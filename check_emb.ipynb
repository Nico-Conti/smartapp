{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "902f558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import supabase_queries as supa\n",
    "\n",
    "importlib.reload(supa)\n",
    "\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import torch, requests\n",
    "from io import BytesIO\n",
    "\n",
    "import os\n",
    "importlib.reload(os)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import json\n",
    "\n",
    "from IPython.display import display, Image as IPythonImage \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67975a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = CLIPModel.from_pretrained(\"patrickjohncyh/fashion-clip\")\n",
    "proc  = CLIPProcessor.from_pretrained(\"patrickjohncyh/fashion-clip\")\n",
    "device = \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def load_img(url):\n",
    "    img = Image.open(BytesIO(requests.get(url, timeout=20).content)).convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "def clip_embed(images=None, texts=None):\n",
    "    inputs = proc(text=texts, images=images, return_tensors=\"pt\", padding=True, max_length=77,truncation=True)\n",
    "    with torch.no_grad():\n",
    "        out = model(**{k: v.to(device) for k,v in inputs.items()})\n",
    "    img = out.image_embeds if images is not None else None\n",
    "    txt = out.text_embeds  if texts is not None else None\n",
    "    if img is not None: img = torch.nn.functional.normalize(img, dim=-1)\n",
    "    if txt is not None: txt = torch.nn.functional.normalize(txt, dim=-1)\n",
    "    return img, txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7af24b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = CLIPModel.from_pretrained(\"patrickjohncyh/fashion-clip\")\n",
    "proc  = CLIPProcessor.from_pretrained(\"patrickjohncyh/fashion-clip\")\n",
    "device = \"cpu\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def load_img(url):\n",
    "    # Add a User-Agent header to mimic a web browser\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    # Make the request with the headers\n",
    "    response = requests.get(url, headers=headers, timeout=20)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code != 200:\n",
    "        raise ConnectionError(f\"Request failed with status code {response.status_code} for URL: {url}\")\n",
    "        \n",
    "    # Open the image from the content\n",
    "    img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    return img\n",
    "\n",
    "def clip_embed(images=None, texts=None):\n",
    "    inputs = proc(text=texts, images=images, return_tensors=\"pt\", padding=True, max_length=77,truncation=True)\n",
    "    with torch.no_grad():\n",
    "        out = model(**{k: v.to(device) for k,v in inputs.items()})\n",
    "    img = out.image_embeds if images is not None else None\n",
    "    txt = out.text_embeds  if texts is not None else None\n",
    "    if img is not None: img = torch.nn.functional.normalize(img, dim=-1)\n",
    "    if txt is not None: txt = torch.nn.functional.normalize(txt, dim=-1)\n",
    "    return img, txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55384782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the query vector is a NumPy array (done in step 1)\n",
    "\n",
    "item_1_url = \"https://image.hm.com/assets/hm/45/c5/45c538c59a9386480da9acd855d2dcff99591a1f.jpg?imwidth=768\"\n",
    "item_2_url = \"https://static.zara.net/assets/public/1662/d7fc/3a8244978a1d/42a4424c9aa2/04365420822-e1/04365420822-e1.jpg?ts=1738841337155&\"\n",
    "\n",
    "query = \"A pair of straight jeans\"\n",
    "\n",
    "\n",
    "img_1 = load_img(item_1_url)\n",
    "img_emb_1, query_emb = clip_embed(images=[img_1], texts=[query])\n",
    "\n",
    "img_2 = load_img(item_2_url)\n",
    "img_emb_2, _ = clip_embed(images=[img_2], texts=[query])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9169260e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity (Item 1 vs Query): 0.1513\n",
      "Similarity (Item 2 vs Query): 0.3156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11134/2083302350.py:11: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  dot_product = np.dot(vec_a, vec_b)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_cosine_similarity(vec_a, vec_b):\n",
    "    \"\"\"Calculates the cosine similarity between two NumPy vectors.\"\"\"\n",
    "    # Ensure a single dimension for the dot product if necessary, \n",
    "    # though numpy.dot usually handles this.\n",
    "    vec_a = vec_a.squeeze()\n",
    "    vec_b = vec_b.squeeze()\n",
    "\n",
    "    # Dot product\n",
    "    dot_product = np.dot(vec_a, vec_b)\n",
    "    \n",
    "    # Magnitudes (Euclidean norms)\n",
    "    norm_a = np.linalg.norm(vec_a)\n",
    "    norm_b = np.linalg.norm(vec_b)\n",
    "    \n",
    "    # Check for zero vector to prevent division by zero\n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        return 0.0 # Or raise an error, but 0.0 is common for similarity\n",
    "        \n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "# 1. Similarity for Item 1\n",
    "similarity_1 = calculate_cosine_similarity(img_emb_1, query_emb)\n",
    "print(f\"Similarity (Item 1 vs Query): {similarity_1:.4f}\")\n",
    "\n",
    "# 2. Similarity for Item 2\n",
    "similarity_2 = calculate_cosine_similarity(img_emb_2, query_emb)\n",
    "print(f\"Similarity (Item 2 vs Query): {similarity_2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
